# -*- coding: utf-8 -*-
"""notebook_submissions_habibfabriarrosyid

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nykFPldDdxtstKnV22cOH491znL4XZVM

# Proyek Klasifikasi Gambar: [Input Nama Dataset]
- **Nama:** Habib Fabri Arrosyid
- **Email:** habibarrsyd@gmail.com
- **ID Dicoding:** habibarrsyd

## Import Semua Packages/Library yang Digunakan
"""

import zipfile
import os
import shutil
import random
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing import image
from google.colab import files
from PIL import Image
from tensorflow.keras.models import load_model

!pip install tensorflowjs

"""## Data Preparation

### Data Loading
"""

# Lokasi file zip dan lokasi ekstrak
zip_file = '/content/archive (8).zip'
extract_folder = '/content/satelit'

# Mengekstrak file zip
with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall(extract_folder)

print(f"Dataset telah diekstrak ke {extract_folder}")

"""### Data Preprocessing

#### Split Dataset
"""

# Tentukan path untuk folder train, validation, dan test
train_dir = '/content/satelit/data/train'
val_dir = '/content/satelit/data/validation'
test_dir = '/content/satelit/data/test'

# Membuat folder untuk train, validation, dan test jika belum ada
os.makedirs(train_dir, exist_ok=True)
os.makedirs(val_dir, exist_ok=True)
os.makedirs(test_dir, exist_ok=True)

# Tentukan path ke folder buah
base_folder = '/content/satelit/data'

# Daftar folder kelas buah setelah ekstraksi
class_names = os.listdir(base_folder)

# Membagi gambar menjadi data train, validation, dan test
def split_data(class_name, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    class_folder = os.path.join(base_folder, class_name)
    all_images = os.listdir(class_folder)

    # Mengacak gambar
    random.shuffle(all_images)

    # Hitung jumlah data untuk masing-masing set
    total_images = len(all_images)
    train_size = int(total_images * train_ratio)
    val_size = int(total_images * val_ratio)

    # Memindahkan gambar ke folder train, validation, dan test
    for i, image_name in enumerate(all_images):
        src = os.path.join(class_folder, image_name)

        # Pastikan ini adalah file, bukan folder
        if os.path.isfile(src):
            if i < train_size:
                dst = os.path.join(train_dir, class_name, image_name)
            elif i < train_size + val_size:
                dst = os.path.join(val_dir, class_name, image_name)
            else:
                dst = os.path.join(test_dir, class_name, image_name)

            # Membuat subfolder untuk kelas jika belum ada
            os.makedirs(os.path.dirname(dst), exist_ok=True)

            shutil.copy(src, dst)

# Membagi data untuk setiap kelas
for class_name in class_names:
    split_data(class_name)

# Normalisasi dan augmentasi untuk data latih
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Hanya normalisasi untuk data validasi
validation_datagen = ImageDataGenerator(rescale=1./255)

# Hanya normalisasi untuk data test
test_datagen = ImageDataGenerator(rescale=1./255)

# Menggunakan flow_from_directory untuk data latih
train_generator = train_datagen.flow_from_directory(
    '/content/satelit/data/train',
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical'
)

# Menggunakan flow_from_directory untuk data validasi
validation_generator = validation_datagen.flow_from_directory(
    '/content/satelit/data/validation',
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical'
)

# Menggunakan flow_from_directory untuk data test
test_generator = test_datagen.flow_from_directory(
    '/content/satelit/data/test',
    target_size=(150, 150),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

"""## Modelling"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam

# Membangun model CNN dari scratch
model_scratch = Sequential()

# Layer konvolusi pertama
model_scratch.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
model_scratch.add(MaxPooling2D(pool_size=(2, 2)))

# Layer konvolusi kedua
model_scratch.add(Conv2D(64, (3, 3), activation='relu'))
model_scratch.add(MaxPooling2D(pool_size=(2, 2)))

# Layer konvolusi ketiga
model_scratch.add(Conv2D(128, (3, 3), activation='relu'))
model_scratch.add(MaxPooling2D(pool_size=(2, 2)))

# Flattening untuk memasukkan data ke layer Dense
model_scratch.add(Flatten())

# Fully connected layer
model_scratch.add(Dense(512, activation='relu'))
model_scratch.add(Dropout(0.5))
model_scratch.add(Dense(4, activation='softmax'))

# Kompilasi model
model_scratch.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])

# Melihat ringkasan model
model_scratch.summary()

"""## Evaluasi dan Visualisasi"""

from tensorflow.keras.callbacks import Callback

class StopTrainingAtAccuracy(Callback):
    def __init__(self, target=0.92):
        super(StopTrainingAtAccuracy, self).__init__()
        self.target = target

    def on_epoch_end(self, epoch, logs=None):
        val_acc = logs.get("val_accuracy")
        if val_acc is not None:
            if val_acc >= self.target:
                print(f"\nâœ… Target val_accuracy {self.target*100:.0f}% tercapai, menghentikan pelatihan.")
                self.model.stop_training = True

stop_callback = StopTrainingAtAccuracy(target=0.92)

history_scratch = model_scratch.fit(
    train_generator,
    steps_per_epoch=train_generator.samples // train_generator.batch_size,
    epochs=50,
    validation_data=validation_generator,
    validation_steps=validation_generator.samples // validation_generator.batch_size,
    callbacks=[stop_callback]
)

# Evaluasi pada data train
train_loss, train_accuracy = model_scratch.evaluate(
    train_generator,
    steps=train_generator.samples // train_generator.batch_size
)

# Evaluasi pada data validasi
val_loss, val_accuracy = model_scratch.evaluate(
    validation_generator,
    steps=validation_generator.samples // validation_generator.batch_size
)

# Evaluasi pada data test
test_loss, test_accuracy = model_scratch.evaluate(
    test_generator,
    steps=test_generator.samples // test_generator.batch_size
)

# Tampilkan hasil evaluasi
print("\nðŸ“Š Evaluasi Akhir Model:")
print(f"Train Accuracy   : {train_accuracy * 100:.2f}%")
print(f"Validation Accuracy: {val_accuracy * 100:.2f}%")
print(f"Test Accuracy    : {test_accuracy * 100:.2f}%")

# Plot loss
plt.plot(history_scratch.history['loss'], label='Train Loss')
plt.plot(history_scratch.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.plot(history_scratch.history['accuracy'], label='Train Accuracy')
plt.plot(history_scratch.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# 1. Ambil prediksi dari model
test_generator.reset()  # Penting untuk menghindari shuffle atau batch yang tersisa
pred_probs = model_scratch.predict(test_generator, steps=test_generator.samples // test_generator.batch_size + 1)
pred_classes = np.argmax(pred_probs, axis=1)

# 2. Ambil label asli
true_classes = test_generator.classes
class_labels = list(test_generator.class_indices.keys())

# 3. Confusion matrix
cm = confusion_matrix(true_classes, pred_classes)

# 4. Plot confusion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# 5. Print classification report
print("Classification Report:")
print(classification_report(true_classes, pred_classes, target_names=class_labels))

"""## Konversi Model"""

# === 1. Simpan Model dalam Format SavedModel ===
saved_model_dir = '/content/satelit/saved_model'
os.makedirs(saved_model_dir, exist_ok=True)  # Buat folder jika belum ada

# Simpan dalam format SavedModel
model_scratch.export(saved_model_dir)

print("âœ… Model berhasil disimpan dalam format SavedModel.")

# === 2. Konversi ke TF-Lite ===
tflite_dir = '/content/satelit/tflite'
os.makedirs(tflite_dir, exist_ok=True)

# Inisialisasi konverter TF-Lite dari SavedModel
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)

# Optimasi ukuran model (opsional)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

# Konversi
tflite_model = converter.convert()

# Simpan ke file
with open(os.path.join(tflite_dir, 'model.tflite'), 'wb') as f:
    f.write(tflite_model)
print("âœ… Model berhasil dikonversi dan disimpan dalam format TF-Lite.")

# Simpan label kelas (opsional)
class_labels = list(train_generator.class_indices.keys())
with open(os.path.join(tflite_dir, 'label.txt'), 'w') as f:
    for label in class_labels:
        f.write(f"{label}\n")
print("âœ… Label kelas berhasil disimpan dalam label.txt.")

# === 3. Konversi ke TensorFlow.js ===
tfjs_dir = '/content/satelit/tfjs'
os.makedirs(tfjs_dir, exist_ok=True)

# Pastikan tensorflowjs terinstal (komentar jika sudah terinstall)
# %pip install tensorflowjs

import tensorflowjs as tfjs
# Konversi model Keras (bukan SavedModel) ke format TFJS
tfjs.converters.save_keras_model(model_scratch, tfjs_dir)
print("âœ… Model berhasil disimpan dalam format TensorFlow.js.")

"""## Inference (Optional)"""

# === 1. Load TFLite model ===
tflite_model_path = '/content/satelit/tflite/model.tflite'

# Inisialisasi interpreter TFLite
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter.allocate_tensors()

# Ambil detail input/output
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Ukuran input model (biasanya [1, 224, 224, 3])
input_shape = input_details[0]['shape']

# === 2. Load class labels ===
with open('/content/satelit/tflite/label.txt', 'r') as f:
    class_labels = [line.strip() for line in f.readlines()]

# === 3. Upload dan uji gambar ===
uploaded = files.upload()

for filename in uploaded.keys():
    print(f"\nðŸ” Menguji gambar: {filename}")

    # Baca gambar dan ubah ukurannya sesuai model
    img = Image.open(filename).convert('RGB')
    img = img.resize((input_shape[1], input_shape[2]))
    img_array = image.img_to_array(img)
    img_array = img_array / 255.0  # normalisasi sesuai train_datagen
    img_array = np.expand_dims(img_array, axis=0).astype(np.float32)

    # Set input ke interpreter
    interpreter.set_tensor(input_details[0]['index'], img_array)

    # Inference
    interpreter.invoke()

    # Ambil hasil output
    output_data = interpreter.get_tensor(output_details[0]['index'])
    predicted_index = np.argmax(output_data)
    predicted_label = class_labels[predicted_index]
    confidence = output_data[0][predicted_index]

    print(f"âœ… Prediksi: {predicted_label} (confidence: {confidence:.2f})")

!pip freeze > requirements.txt
print("âœ… File requirements.txt berhasil dibuat.")

import shutil
from google.colab import files

# Specify the folder you want to download
folder_path = '/content/satelit'

# Specify the name for the zip file
zip_name = 'Submissions_Habib_Fabri_Arrosyid_Image_Processing'

# Zip the folder
shutil.make_archive(zip_name, 'zip', folder_path)

# Download the zip file
files.download(f'{zip_name}.zip')

